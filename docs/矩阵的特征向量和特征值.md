# 矩阵的特征向量和特征值

[TOC]

## 线性变换与矩阵的特征向量特征值

线性变换是指一个n维向量被左乘一个n阶矩阵后得到另一个n维向量，它是同维向量空间中的把一-个向量线性映射成了另一个向量。即
$$
y = Ax(x,y \in R^n,A \in R^{n \times n})
$$
如果对于数 $\lambda$，存在一个n维列向量 $u$ (即 $u\in R^n$)，使得：
$$
Au = \lambda u
$$
则称数 $\lambda$ 为矩阵A的一个特征值，u为矩阵A对应于特征值 $\lambda$ 的特征向量。在线性代数中研究线性变换就是研究相应的矩阵A，矩阵A的特征向量和特征值是线性变换研究的重要内容。

## 在数学上的意义

矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。

如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。这里可以将特征值为负，特征向量旋转180度，也可看成方向不变，伸缩比为负值。所以特征向量也叫线性不变量。

特征向量的不变性是他们变成了与其自身共线的向量，他们所在的直线在线性变换下保持不变；

特征向量和他的变换后的向量们在同一根直线上，变换后的向量们或伸长或缩短，或反向伸长或反向缩短，甚至变成零向量(特征值为零时)。

## 在物理上的意义

一个物理系统，其特性可以被一个矩阵所描述，那么这个系统的物理特性就可以被这个矩阵的特征值所决定，这个矩阵能形成“频率的谱”，就是因为矩阵在特征向量所指的方向上具有对向量产生恒定的变换作用：增强(或减弱)特征向量的作用。

进一步的，如果矩阵持续地叠代作用于向量，那么特征向量的就会凸现出来。

各种不同的信号(向量)进入这个系统中后，系统输出的信号(向量)就会发生相位滞后、放大、缩小等各种纷乱的变化。但只有特征信号(特征向量)被稳定的发生放大(或缩小)的变化。如果把系统的输出端口接入输入端口，那么只有特征信号(特征向量)第二次被放大(或缩小)了，其他的信号如滞后的可能滞后也可能超前。

- 例如一个驻波通过一条绳子，绳子上面的每个点组成一个无穷维的向量，这个向量的特征向量就是特征函数sin(t)因为是时变的，就成了特征函数。每个点特征值就是每个点在特定时刻的sin(x+t)取值。
- 再如，从太空中某个角度看地球自转，虽然每个景物的坐标在不断的变换，但是这种变换关于地球的自传轴有对称性，也就是关于此轴的平移和拉伸的坐标变换不敏感。所以地球自转轴，是地球自转这种空间变换的-个特征向量。
- 矩阵的特征向量特征值在材料、力学、电学等方面也有重要的应用。

## 信息处理上的意义

由于这些投影的大小代表了A在特征空间各个分量的投影，那么我们可以使用最小二乘法，求出投影能量最大的那些分量，而把剩下的分量去掉，这样最大限度地保存了矩阵代表的信息，同时可以大大降低矩阵需要存储的维度，简称PCA方法。

线性变换PCA可以用来处理图像。如2维的人像识别：我们把图像A看成矩阵，进一步看成线性变换矩阵,把这个训练图像的特征矩阵求出来(假设取了n个能量最大的特征向量)。

用A乘以这个n个特征向量，得到一个n维矢量a，也就是A在特征空间的投影。今后在识别的时候同一类的图像(例如，来自同一个人的面部照片)，认为是A的线性相关图像，它乘以这个特征向量，得到n个数字组成的一个矢量b，也就是B在特征空间的投影。那么a和b之间的距离就是我们判断B是不是A的准则。

又如Google公司的PageRank，也是通过计算一个用矩阵表示的图。这个图代表了整个Web各个网页“节点”之间的关联。用特征向量来对每一个节点打“特征值”分。